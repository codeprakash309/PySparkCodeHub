{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BLzmZRNGApHo",
        "O0w2c0l5B3QW",
        "9cBFX5LiDVC1",
        "wsR3LpYwJZht",
        "PKuaPETcJqg2"
      ],
      "authorship_tag": "ABX9TyMUptaeOrPaFqtEq9nMxwXL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***50 PySpark problems & Solutions***"
      ],
      "metadata": {
        "id": "2NUUf5HdAez5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***1. Remove duplicates from a dataset containing customer data***"
      ],
      "metadata": {
        "id": "BLzmZRNGApHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "gk-QFWM7AkCB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"RemoveDuplicates\").getOrCreate()"
      ],
      "metadata": {
        "id": "JRPJeVUvAynh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Dataframe Structure*"
      ],
      "metadata": {
        "id": "TrV-6FnkBUsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"Alice\", 30), (\"Bob\", 25), (\"Alice\", 30),(\"Prakash\",25)]\n",
        "columns = [\"Name\", \"Age\"]\n"
      ],
      "metadata": {
        "id": "gcKAgoJwBEEY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data, columns)"
      ],
      "metadata": {
        "id": "B3rwtOOnBJkh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The below code will remove the duplicate value from the dataframe ***"
      ],
      "metadata": {
        "id": "rWSX_gtoBgLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_no_duplicates = df.dropDuplicates()\n",
        "df_no_duplicates.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjdWu9eIBcY0",
        "outputId": "d3760089-4645-40e3-927c-1578b232bd79"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|    Bob| 25|\n",
            "|  Alice| 30|\n",
            "|Prakash| 25|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wMsuGky2Ad8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***2. Calculate the average salary of employees***"
      ],
      "metadata": {
        "id": "O0w2c0l5B3QW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the SparkSession\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "zpdIXX8HB70h"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the App name\n",
        "spark = SparkSession.builder.appName(\"AverageSalary\").getOrCreate()"
      ],
      "metadata": {
        "id": "l9m2hGojCNIV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Below code has the column name  along with data\n",
        "data = [(\"Alice\", 3000), (\"Bob\", 4000), (\"Charlie\", 5000)]\n",
        "columns = [\"Name\", \"Salary\"]\n",
        "df = spark.createDataFrame(data, columns)"
      ],
      "metadata": {
        "id": "E7MXzd7LCbUg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_salary = df.agg({\"Salary\": \"avg\"}).collect()[0][0]\n",
        "print(f\"Average Salary: {average_salary}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nf4TIE3oCxzd",
        "outputId": "b61e7b2c-3cd8-4197-e08a-405f5310da7e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Salary: 4000.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***3. Join two datasets (employee and department details)***"
      ],
      "metadata": {
        "id": "9cBFX5LiDVC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Employees DataFrame***"
      ],
      "metadata": {
        "id": "-tbz_q3TDuls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp_data = [(\"Alice\", 1), (\"Bob\", 2)]\n",
        "emp_columns = [\"Name\", \"DeptID\"]\n",
        "emp_df = spark.createDataFrame(emp_data, emp_columns)"
      ],
      "metadata": {
        "id": "sSkIuKvpDckV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Department DataFrame***"
      ],
      "metadata": {
        "id": "1GoO4R4wD_ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dept_data = [(1, \"HR\"), (2, \"Finance\")]\n",
        "dept_columns = [\"DeptID\", \"DeptName\"]\n",
        "dept_df = spark.createDataFrame(dept_data, dept_columns)"
      ],
      "metadata": {
        "id": "DFCAi4ceEEae"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Joining the two dataframe*"
      ],
      "metadata": {
        "id": "9JcMCNmjE-Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df = emp_df.join(dept_df, \"DeptID\")\n",
        "joined_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY8_lIWoEsXI",
        "outputId": "2afd1a3c-bd20-4f39-a7f1-cb220b1d9dc2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+--------+\n",
            "|DeptID| Name|DeptName|\n",
            "+------+-----+--------+\n",
            "|     1|Alice|      HR|\n",
            "|     2|  Bob| Finance|\n",
            "+------+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Filter records where a column exceeds a threshold***"
      ],
      "metadata": {
        "id": "wsR3LpYwJZht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df[\"Salary\"] > 4000).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDbCLFoTJgrS",
        "outputId": "7308b3d8-129b-44b8-c799-b5434e521e25"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+\n",
            "|   Name|Salary|\n",
            "+-------+------+\n",
            "|Charlie|  5000|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***5. Handle missing values (dropping and imputing)***"
      ],
      "metadata": {
        "id": "PKuaPETcJqg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"Alice\", None), (\"Bob\", 3000), (\"Charlie\", None)]\n",
        "columns = [\"Name\", \"Salary\"]\n",
        "df = spark.createDataFrame(data, columns)"
      ],
      "metadata": {
        "id": "DuZeySPJJsfw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9wODgJmJ3zt",
        "outputId": "510a64a2-478c-49b3-f702-f6df50b81667"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+\n",
            "|   Name|Salary|\n",
            "+-------+------+\n",
            "|  Alice|  NULL|\n",
            "|    Bob|  3000|\n",
            "|Charlie|  NULL|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop missing values\n",
        "df_dropped = df.na.drop()"
      ],
      "metadata": {
        "id": "JFAYTJCHJyoY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dropped.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sG-lJhnJ8mw",
        "outputId": "bc483d95-ed0b-43bd-a631-f80228be77a2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|Name|Salary|\n",
            "+----+------+\n",
            "| Bob|  3000|\n",
            "+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Imputing a value to the None ***"
      ],
      "metadata": {
        "id": "WmbLOJ_zKsxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values\n",
        "df_imputed = df.na.fill({\"Salary\": 4000})\n",
        "df_dropped.show()\n",
        "df_imputed.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQpZoNr5KB2J",
        "outputId": "2adf596e-80ad-44dc-fcfe-da37b9ed3715"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|Name|Salary|\n",
            "+----+------+\n",
            "| Bob|  3000|\n",
            "+----+------+\n",
            "\n",
            "+-------+------+\n",
            "|   Name|Salary|\n",
            "+-------+------+\n",
            "|  Alice|  4000|\n",
            "|    Bob|  3000|\n",
            "|Charlie|  4000|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***6. Count unique words in a large text file***"
      ],
      "metadata": {
        "id": "HSZyOpt4LSSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_rdd = spark.sparkContext.textFile(\"path/to/textfile\")\n",
        "word_counts = (text_rdd.flatMap(lambda line: line.split()) .map(lambda word: (word, 1)) .reduceByKey(lambda a, b: a + b)) word_counts.count()"
      ],
      "metadata": {
        "id": "fiv9p1D8LNZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***8. Repartition a DataFrame for improved parallelism***"
      ],
      "metadata": {
        "id": "DYCm0FaTNEtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_repartitioned = df.repartition(10)\n",
        "df_repartitioned.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky--lEMRM2Q4",
        "outputId": "9e0d30f9-5cca-4966-c9b2-9a176c051624"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+\n",
            "|   Name|Salary|\n",
            "+-------+------+\n",
            "|    Bob|  3000|\n",
            "|Charlie|  NULL|\n",
            "|  Alice|  NULL|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***9. Sliding window function for moving average***"
      ],
      "metadata": {
        "id": "Bz3_qhiKRq-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType,IntegerType\n",
        "import datetime"
      ],
      "metadata": {
        "id": "SaL-I78vRqKH"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"WindowFunctionExample\").getOrCreate()"
      ],
      "metadata": {
        "id": "hjx-_kaPSfc6"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Data\n",
        "data = [\n",
        "    (\"2024-01-01\", 50000),\n",
        "    (\"2024-01-02\", 55000),\n",
        "    (\"2024-01-03\", 60000),\n",
        "    (\"2024-01-04\", 58000),\n",
        "    (\"2024-01-05\", 62000),\n",
        "    (\"2024-01-06\", 63000),\n",
        "    (\"2024-01-07\", 61000),\n",
        "]\n",
        "\n",
        "# Define schema with Salary as IntegerType\n",
        "schema = StructType([\n",
        "    StructField(\"Date\", StringType(), True),\n",
        "    StructField(\"Salary\", IntegerType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "fcj538xlSjin"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema)"
      ],
      "metadata": {
        "id": "qMNsDjSNU4CI"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Date column to DateType\n",
        "from pyspark.sql.functions import to_date\n",
        "df = df.withColumn(\"Date\", to_date(\"Date\", \"yyyy-MM-dd\"))\n",
        "\n",
        "# Define Window Spec: 3-day moving average (current row + 2 preceding)\n",
        "from pyspark.sql.window import Window\n",
        "window_spec = Window.orderBy(\"Date\").rowsBetween(-2, 0)\n",
        "\n",
        "# Add moving average column\n",
        "from pyspark.sql.functions import avg\n",
        "df_with_avg = df.withColumn(\"MovingAvg\", avg(\"Salary\").over(window_spec))\n",
        "\n",
        "# Show result\n",
        "df_with_avg.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4BBU_uTR3ip",
        "outputId": "f5c423f4-7915-47ef-d477-e4f455af6902"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+------------------+\n",
            "|Date      |Salary|MovingAvg         |\n",
            "+----------+------+------------------+\n",
            "|2024-01-01|50000 |50000.0           |\n",
            "|2024-01-02|55000 |52500.0           |\n",
            "|2024-01-03|60000 |55000.0           |\n",
            "|2024-01-04|58000 |57666.666666666664|\n",
            "|2024-01-05|62000 |60000.0           |\n",
            "|2024-01-06|63000 |61000.0           |\n",
            "|2024-01-07|61000 |62000.0           |\n",
            "+----------+------+------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}